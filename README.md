# Applied AI Insight Assistant with Snowflake Cortex
## Snowflake Cortex Streamlit Apps — 30 Days of AI

### Day 1 — Snowflake Connection
Validated Streamlit in Snowflake session and warehouse connectivity.

### Day 2 — LLM Inference with Cortex (SQL)
Executed Claude 3.5 Sonnet using Snowflake Cortex via Snowpark.

### Day 3 — Streaming LLM Responses
Built a real-time LLM streaming interface using Snowflake Cortex.

Key highlights:
- Multi-model selection (Claude, Mistral, LLaMA)
- Token-level streaming for improved UX
- Fallback generator for production compatibility

### Day 4 — Cached LLM Application
Built a production-ready Streamlit app with:
- Cortex LLM inference
- Streamlit caching
- Latency measurement
- Secure, in-warehouse execution

### Day 5 — LinkedIn Post Generator
- Built an AI-powered content generator using Snowflake Cortex
- Dynamic prompt engineering (tone, length, URL-based context)
- Cached LLM calls for performance optimization

### Day 6 — Status UI for Long-Running AI Tasks
Enhanced the LinkedIn Post Generator with:
- Real-time status updates
- Clear AI task progress indicators
- Improved user experience for long-running LLM calls

This pattern mirrors production AI systems where inference latency
must be communicated clearly to users.

